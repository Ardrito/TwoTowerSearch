{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "091158fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import random\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import torch\n",
    "import numpy as np\n",
    "random.seed(42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cf21d025",
   "metadata": {},
   "outputs": [],
   "source": [
    "marco_dataset = load_dataset('ms_marco', 'v1.1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7f52b903",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('tockenizer.pickle', 'rb') as file:\n",
    "    tokenizer = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2bb9528c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(word, tokenizer=tokenizer):\n",
    "    if word in tokenizer.keys():\n",
    "        return tokenizer[word]\n",
    "    else:\n",
    "        return 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5d333362",
   "metadata": {},
   "outputs": [],
   "source": [
    "inverse_tokens = {}\n",
    "def inverse(tokenizer):\n",
    "    for token in tokenizer:\n",
    "\n",
    "        inverse_tokens[tokenizer[token]] = token\n",
    "#inverse(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ac7572bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reverse_tokenize(word,inverse_tokens=inverse_tokens):\n",
    "    if (int(word) == 0):\n",
    "        return None\n",
    "    return inverse_tokens[word]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "443f5095",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_marco = []\n",
    "for split in marco_dataset:\n",
    "    #print (len(marco_dataset))\n",
    "    tokenized_sample = []\n",
    "    for sample in marco_dataset[split]:\n",
    "        data = []\n",
    "        row = []\n",
    "        query_tokens = []\n",
    "        #print (sample['query'])\n",
    "        for word in sample['query'].split(\" \"):\n",
    "            query_tokens.append(np.float32(tokenize(word)))\n",
    "        data.append(np.float32(query_tokens))\n",
    "        data.append(np.float32((sample['passages']['is_selected'])))\n",
    "        #break\n",
    "        for words in (sample['passages']['passage_text']):\n",
    "            words_ = re.sub(r'[^a-zA-Z-\\s]', '', words)\n",
    "            words_ = (words_.lower().split(\" \"))\n",
    "            tockenized_passage_text = []\n",
    "            #print (words)\n",
    "            for word in words_:\n",
    "                #print(word)\n",
    "                #print (len(word))\n",
    "                tockenized_passage_text.append(np.float32(tokenize(word)))\n",
    "            data.append(np.float32(tockenized_passage_text))\n",
    "            #print (len(tockenized_passage_text))\n",
    "        row.append((data))\n",
    "        #print(row[0])\n",
    "        tokenized_sample.append((row[0]))\n",
    "        #print(row)\n",
    "        #break\n",
    "    #break\n",
    "    tokenized_marco.append((tokenized_sample))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "83502e3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print (tokenized_marco[0][0][1])\n",
    "# words = []\n",
    "# for word in (tokenized_marco[0][0][1]):\n",
    "#     #print (int(word))\n",
    "#     words.append(reverse_tokenize(int(word)))\n",
    "# print ((marco_dataset['validation'][0]['passages']['passage_text'][1]).split(\" \"))\n",
    "# print ((words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "f2bb1812",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_marco_val = pd.DataFrame(tokenized_marco[0]).fillna(0)\n",
    "tokenized_marco_train = pd.DataFrame(tokenized_marco[1]).fillna(0)\n",
    "tokenized_marco_test = pd.DataFrame(tokenized_marco[2]).fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "f8249303",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_marco_train.to_csv(\"tokenized_marco_train.csv\")\n",
    "tokenized_marco_test.to_csv(\"tokenized_marco_test.csv\")\n",
    "tokenized_marco_val.to_csv(\"tokenized_marco_val.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
