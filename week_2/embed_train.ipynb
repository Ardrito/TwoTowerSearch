{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "06da8d06",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/TwoTowerSearch/week_2/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import random\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "import torch.optim as optim\n",
    "random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d39a1ee0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating validation split: 100%|██████████| 10047/10047 [00:00<00:00, 35852.63 examples/s]\n",
      "Generating train split: 100%|██████████| 82326/82326 [00:01<00:00, 43461.33 examples/s]\n",
      "Generating test split: 100%|██████████| 9650/9650 [00:00<00:00, 49148.46 examples/s]\n"
     ]
    }
   ],
   "source": [
    "marco_dataset = load_dataset('ms_marco', 'v1.1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "af821ee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('tokenizer.pickle', 'rb') as file:\n",
    "    tokenizer = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1718ee74",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(word, tokenizer=tokenizer):\n",
    "    if word in tokenizer.keys():\n",
    "        return tokenizer[word]\n",
    "    else:\n",
    "        return 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d7c23a55",
   "metadata": {},
   "outputs": [],
   "source": [
    "inverse_tokens = {}\n",
    "def inverse(tokenizer):\n",
    "    for token in tokenizer:\n",
    "\n",
    "        inverse_tokens[tokenizer[token]] = token\n",
    "inverse(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "64e54c1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reverse_tokenize(word,inverse_tokens=inverse_tokens):\n",
    "    if (int(word) == 0):\n",
    "        return None\n",
    "    return inverse_tokens[word]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "beba1555",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_marco_text = []\n",
    "for split in marco_dataset:\n",
    "    tokenized_sample = []\n",
    "    for sample in marco_dataset[split]:\n",
    "        for words in (sample['passages']['passage_text']):\n",
    "            words_ = re.sub(r'[^a-zA-Z-\\s]', '', words)\n",
    "            words_ = (words_.lower().split(\" \"))\n",
    "            for word in words_:\n",
    "                tokenized_marco_text.append(tokenize(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ee035461",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(\"tokenized_marco/tokenized_marco_text\",\"wb\") as file:\n",
    "#     pickle.dump(tokenized_marco_text,file)\n",
    "\n",
    "with open(\"tokenized_marco/tokenized_marco_text\",\"rb\") as file:\n",
    "    tokenized_marco_text = pickle.load(file)  #To load the file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7d788778",
   "metadata": {},
   "outputs": [],
   "source": [
    "class embed_train_dataset(Dataset):\n",
    "    def __init__(self, words, window=2):\n",
    "        self.data = words\n",
    "        self.window = window\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)-4\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        idx = idx+self.window    \n",
    "        sent = self.data[max(0,idx-self.window):min(idx+self.window+1,len(self.data))]    \n",
    "        if len(sent) > 1:\n",
    "            rand_idx = random.randint(0,len(sent)-1)\n",
    "            target = sent[rand_idx]\n",
    "            del sent[rand_idx]\n",
    "            #print (sent)\n",
    "            tokenized = torch.tensor(sent)\n",
    "            #print (tokenized)\n",
    "            \n",
    "            return tokenized, torch.tensor(target)\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e1a93d03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[   68,   706,   302, 15018]]), tensor([139])]\n"
     ]
    }
   ],
   "source": [
    "dataset = embed_train_dataset(tokenized_marco_text)\n",
    "dataloader = DataLoader(dataset, batch_size=1,shuffle=True)\n",
    "\n",
    "for data in dataloader:\n",
    "    print (data)\n",
    "    break\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "84d0021e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "76288"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fc8aaba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CBOW(nn.Module):\n",
    "    def __init__(self, vocab_size = 76288, embedding_dim = 256):\n",
    "        super().__init__()\n",
    "        self.embed = nn.Embedding(vocab_size,embedding_dim)   \n",
    "        self.lin = nn.Linear(embedding_dim,vocab_size)\n",
    "\n",
    "    def forward(self,inputs):\n",
    "        # print (inputs)\n",
    "        # print(inputs.shape)\n",
    "        embs = self.embed(inputs)\n",
    "        embs = embs.mean(dim=1)\n",
    "        out = self.lin(embs)\n",
    "        probs = F.log_softmax(out,dim=1)\n",
    "        return probs\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7762aaf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop():\n",
    "    number_epochs = 100\n",
    "\n",
    "    #train_wiki, val_wiki = train_test_split(words)\n",
    "    os.makedirs(\"checkpoints\", exist_ok=True)\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print (device)\n",
    "    dataset = embed_train_dataset(tokenized_marco_text)\n",
    "    dataloader = DataLoader(dataset, batch_size=256,shuffle=True)\n",
    "    \n",
    "    model = CBOW().to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.003)\n",
    "    best_loss = 100000000000000.0\n",
    "    for epoch in range(number_epochs):\n",
    "        model.train()\n",
    "        epoch_loss = 0.0\n",
    "        for X,Y in tqdm(dataloader):\n",
    "            try:\n",
    "                \n",
    "                    X = X.to(device)\n",
    "                    Y = Y.to(device)\n",
    "                    optimizer.zero_grad()\n",
    "                    pred = model(X)\n",
    "                    loss = F.cross_entropy(pred,Y)\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "                    epoch_loss += loss\n",
    "            except:\n",
    "                 print (X,Y)\n",
    "\n",
    "            #print (loss.item())\n",
    "        epoch_loss = epoch_loss/len(dataloader)\n",
    "        print(f\"Epoch: {epoch}/{number_epochs}, loss: {epoch_loss} \")\n",
    "        if epoch_loss < best_loss:\n",
    "            best_loss = epoch_loss\n",
    "            torch.save(model.state_dict(), f'checkpoints/best.pt')\n",
    "            print(f\"Model improved. Saved.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ba5ad894",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mtrain_loop\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 12\u001b[39m, in \u001b[36mtrain_loop\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m      9\u001b[39m dataset = embed_train_dataset(tokenized_marco_text)\n\u001b[32m     10\u001b[39m dataloader = DataLoader(dataset, batch_size=\u001b[32m256\u001b[39m,shuffle=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m model = \u001b[43mCBOW\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     13\u001b[39m optimizer = optim.Adam(model.parameters(), lr=\u001b[32m0.003\u001b[39m)\n\u001b[32m     14\u001b[39m best_loss = \u001b[32m100000000000000.0\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/TwoTowerSearch/week_2/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1343\u001b[39m, in \u001b[36mModule.to\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1340\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1341\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1343\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/TwoTowerSearch/week_2/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:903\u001b[39m, in \u001b[36mModule._apply\u001b[39m\u001b[34m(self, fn, recurse)\u001b[39m\n\u001b[32m    901\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[32m    902\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.children():\n\u001b[32m--> \u001b[39m\u001b[32m903\u001b[39m         \u001b[43mmodule\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    905\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[32m    906\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m torch._has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[32m    907\u001b[39m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[32m    908\u001b[39m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    913\u001b[39m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[32m    914\u001b[39m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/TwoTowerSearch/week_2/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:930\u001b[39m, in \u001b[36mModule._apply\u001b[39m\u001b[34m(self, fn, recurse)\u001b[39m\n\u001b[32m    926\u001b[39m \u001b[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[32m    927\u001b[39m \u001b[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[32m    928\u001b[39m \u001b[38;5;66;03m# `with torch.no_grad():`\u001b[39;00m\n\u001b[32m    929\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m--> \u001b[39m\u001b[32m930\u001b[39m     param_applied = \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    931\u001b[39m p_should_use_set_data = compute_should_use_set_data(param, param_applied)\n\u001b[32m    933\u001b[39m \u001b[38;5;66;03m# subclasses may have multiple child tensors so we need to use swap_tensors\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/TwoTowerSearch/week_2/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1329\u001b[39m, in \u001b[36mModule.to.<locals>.convert\u001b[39m\u001b[34m(t)\u001b[39m\n\u001b[32m   1322\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m convert_to_format \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m t.dim() \u001b[38;5;129;01min\u001b[39;00m (\u001b[32m4\u001b[39m, \u001b[32m5\u001b[39m):\n\u001b[32m   1323\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m t.to(\n\u001b[32m   1324\u001b[39m             device,\n\u001b[32m   1325\u001b[39m             dtype \u001b[38;5;28;01mif\u001b[39;00m t.is_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t.is_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1326\u001b[39m             non_blocking,\n\u001b[32m   1327\u001b[39m             memory_format=convert_to_format,\n\u001b[32m   1328\u001b[39m         )\n\u001b[32m-> \u001b[39m\u001b[32m1329\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mt\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1330\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1331\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m.\u001b[49m\u001b[43mis_floating_point\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m.\u001b[49m\u001b[43mis_complex\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   1332\u001b[39m \u001b[43m        \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1333\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1334\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m   1335\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(e) == \u001b[33m\"\u001b[39m\u001b[33mCannot copy out of meta tensor; no data!\u001b[39m\u001b[33m\"\u001b[39m:\n",
      "\u001b[31mRuntimeError\u001b[39m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "train_loop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
